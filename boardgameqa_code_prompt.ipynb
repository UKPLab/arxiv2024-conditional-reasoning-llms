{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.boardgameqa.code_prompt import CodePrompt\n",
    "from src.boardgameqa.evaluation import evaluate\n",
    "from src.utils import get_current_time, print_chain_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THESE VARIABLES\n",
    "dataset_name = \"BoardgameQA-Main-depth1\"\n",
    "num_dev_examples = 500  # number of examples to evaluate on the dev set\n",
    "num_translation_demonstrations = 4\n",
    "num_interpreter_demonstrations = 3\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo-16k-0613\"\n",
    "start_idx = 0  # starting index to evaluate of the dev set. Only modify if you want to skip some examples\n",
    "end_idx = start_idx + num_dev_examples\n",
    "save_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use Azure OpenAI Service\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=llm_name,\n",
    "    temperature=0.0,\n",
    "    request_timeout=30,\n",
    "    max_retries=3,\n",
    "    timeout=60 * 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use OpenAI API\n",
    "openai_api_key = \"\"\n",
    "llm = ChatOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    model=llm_name,\n",
    "    temperature=0.0,\n",
    "    request_timeout=30,\n",
    "    max_retries=3,\n",
    "    timeout=60 * 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS. DO NOT CHANGE\n",
    "data_path = \"data/BoardgameQA/\"\n",
    "\n",
    "with open(os.path.join(data_path, dataset_name, \"ICL_examples\", \"code.json\")) as f:\n",
    "    icl_examples = json.load(f)\n",
    "\n",
    "with open(os.path.join(data_path, dataset_name, \"valid.json\")) as f:\n",
    "    valid = json.load(f)\n",
    "\n",
    "# making output path\n",
    "output_path = os.path.join(\n",
    "    \"outputs/boardgameqa\",\n",
    "    dataset_name,\n",
    "    \"ICL/CodePrompt\",\n",
    "    llm_name,\n",
    "    f\"ICL_transl{num_translation_demonstrations}_interp{num_interpreter_demonstrations}\",\n",
    "    f\"valid{start_idx}_{end_idx}\",\n",
    ")\n",
    "\n",
    "# by default seed = 0. But it will be set to the number of runs if save_results = True\n",
    "seed = 0\n",
    "if save_results:\n",
    "    # creating the base folder\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    # creating the run folder\n",
    "    num_runs = len(glob(os.path.join(output_path, \"*\")))\n",
    "    seed = num_runs\n",
    "    output_path = os.path.join(output_path, f\"run_{num_runs}\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    print(f\"Output path: {output_path}\")\n",
    "random.seed(seed)\n",
    "\n",
    "# creating model\n",
    "model = CodePrompt(\n",
    "    icl_examples,\n",
    "    llm,\n",
    "    num_translation_demonstrations=num_translation_demonstrations,\n",
    "    num_interpreter_demonstrations=num_interpreter_demonstrations,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "openai_metadata = {\n",
    "    \"completion_tokens\": [],\n",
    "    \"total_cost\": [],\n",
    "    \"total_tokens\": [],\n",
    "    \"prompt_tokens\": [],\n",
    "}\n",
    "\n",
    "list_answers = []\n",
    "list_input_prompts = []\n",
    "pbar = tqdm(valid[start_idx:end_idx])\n",
    "for idx, x in enumerate(pbar):\n",
    "    with get_openai_callback() as cb:\n",
    "        input_text = model.create_input_text(x)\n",
    "        try:\n",
    "            response, code, input_prompt = model(input_text)\n",
    "        except Exception as e:\n",
    "            response = \"\"\n",
    "            code = \"\"\n",
    "            print(f\"Error in example {idx}.\", e)\n",
    "        openai_metadata[\"completion_tokens\"].append(cb.completion_tokens)\n",
    "        openai_metadata[\"total_cost\"].append(cb.total_cost)\n",
    "        openai_metadata[\"total_tokens\"].append(cb.total_tokens)\n",
    "        openai_metadata[\"prompt_tokens\"].append(cb.prompt_tokens)\n",
    "\n",
    "    list_answers.append(\n",
    "        {\n",
    "            \"idx\": idx,\n",
    "            \"response\": response,\n",
    "            \"answer\": model.process_response(response),\n",
    "            \"code\": code,\n",
    "        }\n",
    "    )\n",
    "    list_input_prompts.append({\"idx\": idx, \"input_prompt\": input_prompt})\n",
    "    pbar.set_description(\n",
    "        f\"Current total cost: {sum(openai_metadata['total_cost']):.2f}\"\n",
    "    )\n",
    "\n",
    "# evaluation\n",
    "list_predictions = [x[\"answer\"] for x in list_answers]\n",
    "results = evaluate(valid[start_idx:end_idx], list_predictions)\n",
    "\n",
    "if save_results:\n",
    "    # store outputs\n",
    "    with open(os.path.join(output_path, \"output.json\"), \"w\") as f:\n",
    "        json.dump(list_answers, f)\n",
    "    # store input prompts\n",
    "    with open(os.path.join(output_path, \"input_prompts.pkl\"), \"wb\") as f:\n",
    "        pkl.dump(list_input_prompts, f)\n",
    "    # store openai metadata\n",
    "    with open(os.path.join(output_path, \"openai_metadata.json\"), \"w\") as f:\n",
    "        json.dump(openai_metadata, f, indent=4)\n",
    "    # store results\n",
    "    with open(os.path.join(output_path, \"results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    # store timestamp\n",
    "    with open(os.path.join(output_path, \"timestamp.txt\"), \"w\") as f:\n",
    "        f.write(get_current_time())\n",
    "\n",
    "print(\"## OpenAI Metadata ##\")\n",
    "print_chain_stats(openai_metadata)\n",
    "print(\"\\n\\n## Results ##\")\n",
    "print(json.dumps(results, indent=4))\n",
    "print(\"\\n\\n## Timestamp ##\")\n",
    "print(get_current_time())\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = results[\"confusion_matrix\"]\n",
    "cm = np.array(cm)\n",
    "cm = cm / np.sum(cm, axis=1, keepdims=True)\n",
    "# classes are No, Unknown, Yes\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"No\", \"Unknown\", \"Yes\"],\n",
    "    yticklabels=[\"No\", \"Unknown\", \"Yes\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "# save figure\n",
    "if save_results:\n",
    "    plt.savefig(os.path.join(output_path, \"confusion_matrix.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
